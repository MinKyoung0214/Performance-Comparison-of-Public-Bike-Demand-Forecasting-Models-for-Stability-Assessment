# -*- coding: utf-8 -*-
"""Bike sharing project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ObLaERqv_X72cCLZpd3HwmTaTTonsGyP

# 프로젝트 목적


 렌트 수에 영향을 미치는 요소를 파악하여 급작스러운 상황에 대비하기 위해 렌트 수에 영향을 미치는 요소를 파악하여 사업 안정성 상승
"""

!pip uninstall pandas-profiling
!pip install ydata-profiling

!pip install shap

# Commented out IPython magic to ensure Python compatibility.
# library for feature engineering and EDA
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from mpl_toolkits.mplot3d import proj3d
from IPython.display import Image
from ydata_profiling import ProfileReport
from datetime import datetime
import random

# library for statistic
from scipy import stats
from scipy.stats import kruskal
from scipy.stats import boxcox, norm, yeojohnson
from scipy.stats import skew
from scipy.stats import kurtosis
from scipy.stats import uniform as sp_randFloat
from scipy.stats import randint as sp_randInt
from statsmodels.stats.outliers_influence import variance_inflation_factor

# library for machine learning
import sklearn
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler

from sklearn.cluster import DBSCAN

from sklearn.model_selection import train_test_split
from sklearn.utils.class_weight import compute_sample_weight
from sklearn.metrics import mean_squared_error
from sklearn.metrics import roc_auc_score
from sklearn.metrics import precision_recall_fscore_support

from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import LinearSVR
from sklearn.neural_network import MLPRegressor
from sklearn.tree import DecisionTreeRegressor

from xgboost import XGBRegressor

import shap

# %matplotlib inline

"""데이터 로드"""

df = pd.read_csv("/content/bikesharing_hour.csv")

df

#중복 데이터 확인(수집 데이터 정확성 검토)
print("중복된 항목 수 :", len(df[df.duplicated()]))

"""데이터 확인"""

df.info()

for column_name in list(df.columns):
    print(column_name, df[column_name].dtype, df[column_name].unique())

#머신러닝 모델 학습 전에 명시적 인코딩(수치형을 범주형으로)
list_cast_to_object = ["season", "yr", "mnth", "hr", "holiday", "weekday", "workingday", "weathersit"]
for column_name in list_cast_to_object:
  df[column_name] = df[column_name].astype(object)

list_categorical_columns = list(df.select_dtypes(include=['object']).columns)
list_numeric_columns = list(df.select_dtypes(include=['float64','int64']).columns)
target_column = "cnt"
print(len(df))
print(len(df.columns))
print(len(list_categorical_columns))
print(len(list_numeric_columns))

df.isna().sum().sort_values(ascending=False)

"""#종속 변수 데이터 탐색"""

list_numeric_columns.remove(target_column)

df[target_column].describe()

plt.figure(figsize=(13,7))
sns.violinplot(x=target_column, data=df)
plt.show()

plt.figure(figsize=(13,7))
sns.boxplot(x=target_column, data=df)
plt.show()

"""#종속 변수 데이터 탐색

범주형 데이터 분석
"""

df[list_categorical_columns].nunique().sort_values()

# dteday가	731로 너무 많음
# 요일,월,season 데이터가 있기에 dteday 제거
df = df.drop("dteday", axis=1)
list_categorical_columns.remove("dteday")

plt.figure(figsize=(15,15))
x = 1
plt.subplots_adjust(top = 0.99, bottom=0.01, hspace=0.3, wspace=0.2)
for column_name in list_categorical_columns:
    plt.subplot(3,3,x)
    x = x+1
    df[column_name].value_counts().sort_index().plot(kind='bar')
    plt.title(column_name)
plt.show()

"""타켓변수와 범주형 변수 관계 분포 확인"""

# categorical column과 dependent data(target column) 분포 분석
plt.figure(figsize=(16,16))
x = 1
plt.subplots_adjust(top = 0.99, bottom=0.01, hspace=0.2, wspace=0.2)
for column_name in list_categorical_columns:
    plt.subplot(3,3,x)
    x = x+1
    sns.boxplot(data=df, x=column_name, y=target_column)
    plt.title(column_name)
plt.show()

"""정규성 검정

* 귀무가설 : 모집단의 분포는 정규 분포이다.
* 검정 방법 : Shpiro-Wilks Test
* p-value < 0.05이면, 귀무가설 기각
"""

# 자전거 대여 수량이 정규 분포를 따르는지 확인()
stats.probplot(df[target_column], dist=stats.norm, plot=plt)
plt.title("shapiro test' pvalue : %s"%str(stats.shapiro(df[target_column]).pvalue))

"""pvalue : 6.488339028873512e-79  정규성 만족한다고 보기 어려움

Shapiro-Wilk 검정은 소규모 데이터에 더 적합한 검정이므로 Anderson-Darling 검정으로 점검
"""

from scipy.stats import anderson

# 정규성 검정 (기본 분포는 'norm' = 정규분포)
result = anderson(df[target_column], dist='norm')

print("A–D 통계량:", result.statistic)
print("임계값들:", result.critical_values)
print("유의수준들:", result.significance_level)

"""정규성 만족한다고 보기 어려움

#Feature Transformation for Normalization

**boxcox transformation**
"""

transforming_data, y = boxcox((df[target_column]))

stats.probplot(transforming_data, dist=stats.norm, plot=plt)
plt.title("shapiro test' pvalue : %s"%str(stats.shapiro(transforming_data).pvalue))

"""꼬리부분 정규성 만족 어려움(잘라내면 데이터 왜곡을 보일 수 있음)

pvalue : 4.982804937437634e-44 정규성 만족한다고 보기 어려움

**yeo-johnson transformation**
"""

transorming_data_yeo, lmbda = stats.yeojohnson(df[target_column])
stats.probplot(transorming_data_yeo, dist=stats.norm, plot=plt)
plt.title("shapiro test' pvalue : %s"%str(stats.shapiro(transorming_data_yeo).pvalue))

"""꼬리부분 정규성 만족 어려움(잘라내면 데이터 왜곡을 보일 수 있음)

pvalue : 1.7427513229017175e-45 정규성 만족한다고 보기 어려움

**변환을 해도 정규성을 만족하지 않으므로 분산분석 불가**

#Kruskall-Wallis test (비모수 검정)

정규성,등분산성을 만족하지 않아도 되지만  검정 결과 신뢰도가 모수 검정보다 떨어짐
"""

list_categorical_columns

"""통계적으로 유의미한 영향을 주는 범주형 변수

귀무가설 : 각 컬럼의 모든 카테고리에서 cnt의 분포는 동일하다
"""

list_meaningful_column_by_kruskall = []

for column_name in list_categorical_columns:
  list_kruskal = []
  for value in df[column_name].unique():
      df_tmp = df[df[column_name] == value][target_column].dropna()
      list_kruskal.append(np.array(df_tmp))
  statistic, pvalue = kruskal(*list_kruskal)
  if pvalue <= 0.05:
    list_meaningful_column_by_kruskall.append(column_name)
  print(column_name, ", ",statistic,", ", pvalue)
print("all numerical columns : ", len(list_categorical_columns))
print("selected columns by anova : ", len(list_meaningful_column_by_kruskall), list_meaningful_column_by_kruskall)

"""8개 변수 모두 p-value를 보면 0.05 보다 작으므로 귀무가설 기각.

즉 각 컬럼의 모든 카테고리에서 cnt의 분포는 동일하다고 볼 수 없음.
(모두 유의미한 관계를 갖음)

**수치형 데이터 분석**
"""

#불필요한 변수 확인
df[list_numeric_columns].nunique().sort_values()

print(df["instant"].unique())

# 증가하는 unique id같은 데이터이므로  제거
df = df.drop("instant", axis=1)

list_numeric_columns.remove("instant")

df[list_numeric_columns].describe()

plt.figure(figsize=(20,10))
x = 1
plt.subplots_adjust(top = 0.99, bottom=0.01, hspace=0.4, wspace=0.2)
for column_name in list_numeric_columns:
    plt.subplot(3,2,x)
    x = x+1
    sns.violinplot(x=column_name, data=df)
    plt.title(column_name)
plt.show()

plt.figure(figsize=(20,10))
x = 1
plt.subplots_adjust(top = 0.99, bottom=0.01, hspace=0.4, wspace=0.2)
for column_name in list_numeric_columns:
    plt.subplot(3,2,x)
    x = x+1
    sns.boxplot(x=column_name, data=df)
    plt.title(column_name)
plt.show()

#왜도, 척도 확인
for column_name in list_numeric_columns:
  print(column_name, "skew : ", skew(df[column_name]), "kur : ", kurtosis(df[column_name]) )

"""scaling을 활용한 feature preprocessing 필요해보임(모델 학습시 극단값에 민감해짐)

**다중공선성 확인**

Correlation Analysis
"""

df_corr = df[list_numeric_columns].corr()
df_corr

plt.figure(figsize=(8,8))
sns.heatmap(df_corr, annot=True)

index_corr_over_90 = np.where((abs(df_corr) > 0.9) & (df_corr != 1))
index_corr_over_90

len_corr_over_90 = len(index_corr_over_90[0])
left_columns = df_corr.columns[index_corr_over_90[0]]
right_columns = df_corr.columns[index_corr_over_90[1]]
for index in range(len_corr_over_90):
  print(left_columns[index], "<->", right_columns[index])

list_not_valid = []
for index in range(len_corr_over_90):
    statistic, pvalue = stats.pearsonr(df.loc[:,left_columns[index]], df.loc[:,right_columns[index]])
    print("%s<->%s : %f %f"%(left_columns[index],right_columns[index] , statistic, pvalue))
    if (pvalue > 0.05):
        list_not_valid.append(column_name)
print("end")
print(len(list_not_valid), " is not valids")

# 상관관계가 0.9876 으로 매우 높으므로 atemp 제거
list_removed_by_correlation = ["atemp"]

"""VIF Analysis"""

def caculate_vif(df_target):
  vif = pd.DataFrame()
  vif['VIF_Factor'] = [variance_inflation_factor(df_target.values, i) for i in range(df_target.shape[1])]
  vif['Feature'] = df_target.columns
  return vif

df_vif = df[list_numeric_columns].copy()

caculate_vif(df_vif)

"""atemp와 temp 매우 높게 나옴.(둘 중 하나만 제거 요)"""

df_vif2 = df_vif.drop(["atemp"], axis=1).copy()
final_vif = caculate_vif(df_vif2)

final_vif

list_numeric_feature_by_vif = list(final_vif["Feature"].values)
print(list_numeric_feature_by_vif)
print(len(list_numeric_feature_by_vif))

"""list_removed_by_correlation에서 상관관계가 높아 제거하려던 컬럼들도 vif 분석에서 제거됨

**Target Column과 numeric Column 관계 분포 확인**
"""

list_numeric_columns

plt.figure(figsize=(8,8))
sns.heatmap(df[list_numeric_columns + ["cnt"]].corr(), annot=True)

plt.figure(figsize=(18,15))
x = 1
plt.subplots_adjust(top = 0.99, bottom=0.01, hspace=0.2, wspace=0.4)
for column_name in list_numeric_columns:
    plt.subplot(3,3,x)
    x = x + 1
    sns.scatterplot(data=df,x=target_column,y=column_name)
plt.show()

"""# EDA with pandas_profiling"""

from ydata_profiling import ProfileReport
profile = ProfileReport(df)
profile.to_file("eda_report.html")

"""#Data Processing 2"""

df.isna().sum()

"""변수 선택"""

list_selected_categorical = list_meaningful_column_by_kruskall.copy()
print(list_selected_categorical)
print(list_numeric_feature_by_vif)

df_fs = df[list_selected_categorical + list_numeric_feature_by_vif + [target_column]]
df_fs

df_fs.info()

"""Feature Transformation"""

Y = df_fs[target_column]
X = df_fs.drop([target_column], axis=1)

"""numerical data -> feature scaling(standardization)"""

list_categorical_columns = list(X.select_dtypes(include=['object']).columns)
list_numeric_columns = list(X.select_dtypes(include=['float64','int64']).columns)

X.head()

scaler = StandardScaler()
X.loc[:, list_numeric_columns] = scaler.fit_transform(X[list_numeric_columns])
X.head()

"""categorical column data-> One-hot Encoding"""

cat_cols = X.select_dtypes(include='object').columns
X_base = pd.get_dummies(X, columns=cat_cols)

# 더미 변수들만 int로 변환
bool_cols = X_base.select_dtypes(include='bool').columns
X_base[bool_cols] = X_base[bool_cols].astype(int)
X_base.head()

print(X.shape)
print(X_base.shape)

"""이상치 제거

DBSCAN-밀도 기반 클러스터링(특정 기준 점을 반경 2 안에 점 6개 이상이 있으면 군집으로 인식)
"""

dbscan_model = DBSCAN(eps=2, min_samples=6).fit(X_base)
dbscan_model.labels_

pd.Series(dbscan_model.labels_).value_counts()

"""-1	504 밀도기반 클러스터링 시 어느 군집에도 속하지 못하는 것

이상치(노이즈) 제거시 로버스트한 모델 생성 가능
"""

#이상치 제거
X_base_final = X_base[np.where(dbscan_model.labels_==-1, False, True)]
Y_final = Y[np.where(dbscan_model.labels_==-1, False, True)]

X_base_final = X_base_final.reset_index(drop=True)
Y_final = Y_final.reset_index(drop=True)

"""# 각 모델 별 킥보드 대여 횟수를 예측"""

list_model_type = []
list_model_best_params = []
list_model_rmse = []
cv = 3

X_train, X_validation, y_train, y_validation = train_test_split(X_base_final, Y_final)

# model_learning_with_gridsearch 함수 구현
def model_learning_with_gridsearch(model, hyper_parameter, X, Y, metric="neg_mean_squared_error"):
  grid_cv_opt = GridSearchCV(estimator=model, param_grid=hyper_parameter, cv=cv, scoring=metric)
  grid_cv_opt.fit(X,Y)

  y_prd = grid_cv_opt.predict(X_validation)
  rmse = mean_squared_error(y_validation, y_prd)**0.5

  print("validation -> best model hyper-paramet : ", grid_cv_opt.best_params_)
  print("validation -> best model rmse : ", rmse)

  return grid_cv_opt.best_params_, rmse

"""**Neural Networks with MLPRegressor**"""

hyper_params_mlp = {
    'hidden_layer_sizes': [10, 15],
    'activation': ['relu'],
    'solver': ["sgd", "adam"],
    'learning_rate': ['constant'],
    'learning_rate_init': [0.005, 0.01],
    'power_t': [0.5],
    'alpha': [0.0001],
    'max_iter': [10000],
    'early_stopping': [True],
    'warm_start': [False]
              }
params, score = model_learning_with_gridsearch(MLPRegressor(), hyper_params_mlp, X_train, y_train)
list_model_type.append("Neural Networks with MLPRegressor")
list_model_best_params.append(params)
list_model_rmse.append(score)

"""**Bagging with RandomForestRegressor**"""

hyper_params_bagging = {
    'n_estimators': [100,250,400],
    'criterion': ["squared_error"],
    'max_depth': [5,10],
    'max_features': [0.8 ,1.0]
              }
params, score = model_learning_with_gridsearch(RandomForestRegressor(), hyper_params_bagging, X_train, y_train)
list_model_type.append("Bagging with RandomForestRegressor")
list_model_best_params.append(params)
list_model_rmse.append(score)

"""**Boosting with XGBRegressor**"""

hyper_params_xgboost = {
    'n_estimators': [100,250,400],
    'max_leaves': [3,5,7],
    'max_depth': [5,10],
    "learning_rate" : [0.1, 0.5, 1]
              }
params, score = model_learning_with_gridsearch(XGBRegressor(), hyper_params_xgboost, X_train, y_train)
list_model_type.append("Boosting with XGBRegressor")
list_model_best_params.append(params)
list_model_rmse.append(score)

"""**SVM with LinearSVR**"""

hyper_params_svm = {
    'C': [0.5,1.0, 1.5],
    'tol': [1e-5, 1e-4],
    "random_state" : [1234]
              }
params, score = model_learning_with_gridsearch(LinearSVR(), hyper_params_svm, X_train, y_train)
list_model_type.append("SVM with LinearSVR")
list_model_best_params.append(params)
list_model_rmse.append(score)

"""**Decision Tree with DecisionTreeRegressor**"""

hyper_params_decision = {
    'criterion': ["squared_error", "absolute_error"],
    'max_depth': [3, 5, 10],
    'max_features': [0.6, 0.8 ,1.0]
              }
params, score = model_learning_with_gridsearch(DecisionTreeRegressor(), hyper_params_decision, X_train, y_train)
list_model_type.append("Decision Tree with DecisionTreeRegressor")
list_model_best_params.append(params)
list_model_rmse.append(score)

df_metric = pd.DataFrame({'Model':list_model_type, 'rmse':list_model_rmse})
ax = df_metric.plot.barh(x='Model', y='rmse', rot=0, figsize=(10,10), legend=False)
for bar in ax.patches:
    ax.annotate(format(bar.get_width(), '.4f'),
                   (bar.get_width(), bar.get_y() + bar.get_height() / 2),
                   size=13, xytext=(8, 0),
                   textcoords='offset points')

"""# Model Analysis"""

final_model = LinearSVR(**{'C': 1.0, 'random_state': 1234, 'tol': 1e-05})
final_model.fit(X_train, y_train)

final_model.predict(X_validation)

explainer = shap.Explainer(lambda x : final_model.predict(x), X_validation)
shap_values = explainer(X_validation)

shap.summary_plot(shap_values, X_validation)